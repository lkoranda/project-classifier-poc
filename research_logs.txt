**Classification on Word2Vec**

6.2.17:
- The categories sizes' balancing has been proved to improve the performance of the classifier by ~5-10% (on small content categories)
This is most likely the unwanted behavior hopefully to be eliminated by an increase of the train/test dataset expecially for the little content categories.

8.2.17:
- The best results are unexpectedly reached on "webserver" project dataset having only 1041 documents in database. Might be caused by that the contents of this category have by the most part filled the "sys_content_plaintext" attribute containing free meaningful pieces of text (not proved hypothesis)
Partial mean accuracy on CV (20 stratified splits) for webserver group reaches 88.36%
- The biggest group model of eap train/tested on 5000 document (limited for groups size balancing) gives appx. average accuracy: 25.8%
- Overall mean accuracy on CV (20 stratified splits): 28.59%

Classifier over categories' probabilities:
16.2.17:
- The classification atop the scoring vectors for documents seem to sometimes (however very variably) increase the accuracies.
- The improvement seems to have a good effect
-- We have tried Stochastic Gradient Descent classifiers and Random Forrest Classifier, of which none has resulted in any enlightening performance boost.
-- The use of adjacent layer of classifiers seems to have a positive effect on performance stability - adding another group of content
to classification does not significantly destroy the performance on former categories content
- With more relevant (=bigger) categories, the Random Forrest Classifier in average increases accuracy marginally,
SGD dramatically decreases (-25% with eap-fuse-devstudio) accuracy
- this path will no longer be developed

Standard classification (picking the most probable category for each doc):
- The classification seems to perform usable well on categories with most content (eap:20k, fuse:8k, devstudio:5k).
Performance accuracy of eap-fuse-devstudio (stemming, incl. SO, limited eap) classification: 83.99%, eap-fuse: 90.31% (where accuracy on eap:98.94%, fuse:82.07%)
- This result suggests the importance of future acquisition of more content especially for small-volumed categories

StackOverflow content addition effect:
TODO

Stemming effect:
9.2.17:
- Comparing results on stemmed and un-stemmed dataset (content_categories = ["portal", "amq", "webserver", "fsw"], CV split = 5):
- the best results are now seen for fsw project: mean accuracy of 81.44%, a big difference to webserver: mean of %03.67 (?)
- overall accuracy: 20.34% un-stemmed, 20.92% stemmed
15.2.
- Stemming removal has suggested to have a little effect on classifier performance. When training on un-stemmed texts, the accuracy went down by ~1-2%.

Limiting document size:
16.2.
- Limiting the train/test dataset to documents longer than 3 sentences has decreased the accuracy marginally
(on eap-fuse-devstudio content set by 0.85%)
- Limiting only sentence size has marginally decreased the accuracy as well (~ -0.3%)

16.2.17:
- w2v might vectorize only words (BoW approach?) it is not clear how to combine the raw attributes of the tokens in sentences and documents.
- We have decided to move the classification on vectors straight on Doc2Vec as it seems more suitable for the task

**Classification on Doc2Vec**

24.2.17:
- Began integration of Doc2Vec concept with our datasets, as inspired by
https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb
- doc2vec_wrapper should take care of a vectorization for an arbitrary document and adjacent classifier might serve for the classification of the document
- possibly provide the scoring-among-categories output as a result of regression against categories' 1-vs-all style trained models

3.3.17:
Doc2Vec:
- Doc2Vec has now provides the arbitrary-attribute representation of the documents
- Text preprocessing is left as for the most optimal seen results of Word2Vec
- Logistic Regression (using one-vs-rest, newton-cg solver) integrated into the pipeline as adjacent classifier on Doc2Vec
- in config of Doc2Vec(dbow,d300,n5,mc5,t8) trained in 10 epochs with learning alpha=0.1
gives accuracy of 92.8413% (3 most numerous categories: [eap, fuse, devstudio] - partial accuracies: [0.9634, 0.8923, 0.8261])
- where baseline (most common category (==eap)) accuracy is 63.03%
- decreasing inference params and increasing inference steps (default 3->10) has boosted the overall accuracy by ~1.5%

7.3.17:
TODO: two-layered network as constructed in https://deeplearning4j.org/welldressed-recommendation-engine
- 1. layer: input size, activ: relu
- 2. layer: output categories size, activ: softmax

NN on Doc2Vec:
11.3.17:
- no significant gain from increase of train steps from 2000 to 4000 (gain of 0.2%)
- the optimal (so far) dropout on 0.05